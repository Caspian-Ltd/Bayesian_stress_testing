{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from utils import GraphUtils,ExpUtils\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from tensorflow.python import tf2\n",
    "if not tf2.enabled():\n",
    "    import tensorflow.compat.v2 as tf\n",
    "    tf.enable_v2_behavior()\n",
    "    assert tf2.enabled()\n",
    "from tqdm import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context='talk',font_scale=0.7)\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the structure of the Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = GraphUtils.load_graph(r\"graph_structures/toy_example_structure.xlsx\")\n",
    "GraphUtils.visualise_network(network_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the generated data from the structure.\n",
    "We have not yet open sourced our Bayesian systhetic tool which was used to generate the data here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv('data/toy_data.csv')\n",
    "toy_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the distributions of the idependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder per variable\n",
    "x1_encoder = OneHotEncoder().fit(np.asarray(toy_data['x1']).reshape(-1,1))\n",
    "x2_encoder = OneHotEncoder().fit(np.asarray(toy_data['x2']).reshape(-1,1))\n",
    "x3_encoder = OneHotEncoder().fit(np.asarray(toy_data['x3']).reshape(-1,1))\n",
    "\n",
    "m1_encoder = OneHotEncoder().fit(np.asarray(toy_data['m1']).reshape(-1,1))\n",
    "m2_encoder = OneHotEncoder().fit(np.asarray(toy_data['m2']).reshape(-1,1))\n",
    "\n",
    "y_encoder = OneHotEncoder().fit(np.asarray(toy_data['y']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distributions to be used in the paper:\n",
    "plt.bar(['$x_{11}$','$x_{12}$'],[.3,.7],width=[0.3,.3],color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(['$x_{21}$','$x_{22}$','$x_{23}$'],[.1,.5,.4],width=[0.4,.4,.4],color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(['$x_{31}$','$x_{32}$','$x_{33}$'],[.3,.3,.3],width=[0.4,.4,.4],color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(['$m_{11}$','$m_{12}$','$m_{13}$'],[.5,.3,.2],width=[0.4,.4,.4],color='g')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(['$m_{21}$','$m_{22}$'],[.6,.4],width=[0.3,.3],color='g')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(['$y_{11}$','$y_{12}$'],[.5,.5],width=[0.3,.3],color='g')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = .7\n",
    "x1_dist = tfd.RelaxedOneHotCategorical(temperature, probs=[.3,.7])\n",
    "x2_dist = tfd.RelaxedOneHotCategorical(temperature, probs=[.1,.5,.4])\n",
    "x3_dist = tfd.RelaxedOneHotCategorical(temperature, probs=[.3,.3,.3])\n",
    "m1_prior = tfd.OneHotCategorical(probs=[.5,.3,.2]) \n",
    "m2_prior = tfd.OneHotCategorical(probs=[.6,.4]) \n",
    "y_prior = tfd.OneHotCategorical(probs=[.5,.5])\n",
    "\n",
    "n = toy_data.shape[0]\n",
    "x1_prior = tfd.OneHotCategorical(probs=np.asarray((toy_data['x1'].value_counts()/n)[x1_encoder.categories_[0]]))\n",
    "x2_prior = tfd.OneHotCategorical(probs=np.asarray((toy_data['x2'].value_counts()/n)[x2_encoder.categories_[0]]))\n",
    "x3_prior = tfd.OneHotCategorical(probs=np.asarray((toy_data['x3'].value_counts()/n)[x3_encoder.categories_[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build estimators for the conditional distributions simulating real classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build p(m1|x1,x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_data['m1_lbl'] =  LabelEncoder().fit_transform(toy_data['m1'])\n",
    "x1_feature = x1_encoder.transform(np.asarray(toy_data['x1']).reshape(-1,1)).toarray()\n",
    "x2_feature = x2_encoder.transform(np.asarray(toy_data['x2']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_m1 = np.hstack((x1_encoder.categories_,x2_encoder.categories_))[0]\n",
    "data_m1 = np.hstack((x1_feature,x2_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m1:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m1 = pd.DataFrame(columns = feature_names_m1,data=data_m1)\n",
    "train_input_fn = ExpUtils.make_input_fn(data_m1, toy_data['m1_lbl'])\n",
    "eval_input_fn = ExpUtils.make_input_fn(data_m1, toy_data['m1_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "m1_x1_x2_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=3)\n",
    "m1_x1_x2_est.train(train_input_fn)\n",
    "result = m1_x1_x2_est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results from TF\n",
    "print(result)\n",
    "print('*****************************')\n",
    "pred_dicts = list(m1_x1_x2_est.predict(eval_input_fn))\n",
    "preds = [int(pred['classes'][0]) for pred in pred_dicts]\n",
    "\n",
    "print(classification_report(toy_data['m1_lbl'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build p(m2|x2,x3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build p(m2|x2,x3) \n",
    "toy_data['m2_lbl'] =  LabelEncoder().fit_transform(toy_data['m2'])\n",
    "x2_feature = x2_encoder.transform(np.asarray(toy_data['x2']).reshape(-1,1)).toarray()\n",
    "x3_feature = x3_encoder.transform(np.asarray(toy_data['x3']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_m2 = np.hstack((x2_encoder.categories_,x3_encoder.categories_))[0]\n",
    "data_m2 = np.hstack((x2_feature,x3_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m2:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m2 = pd.DataFrame(columns = feature_names_m2,data=data_m2)\n",
    "train_input_fn_m2 = ExpUtils.make_input_fn(data_m2, toy_data['m2_lbl'])\n",
    "eval_input_fn_m2 = ExpUtils.make_input_fn(data_m2, toy_data['m2_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "m2_x2_x3_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=2)\n",
    "m2_x2_x3_est.train(train_input_fn_m2)\n",
    "result = m2_x2_x3_est.evaluate(eval_input_fn_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results from TF\n",
    "print(result)\n",
    "print('*****************************')\n",
    "pred_dicts = list(m2_x2_x3_est.predict(eval_input_fn_m2))\n",
    "preds = [int(pred['classes'][0]) for pred in pred_dicts]\n",
    "\n",
    "print(classification_report(toy_data['m2_lbl'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build p(y|m1,m2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_data['y_lbl'] =  LabelEncoder().fit_transform(toy_data['y'])\n",
    "m1_feature = m1_encoder.transform(np.asarray(toy_data['m1']).reshape(-1,1)).toarray()\n",
    "m2_feature = m2_encoder.transform(np.asarray(toy_data['m2']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_y = np.hstack((m1_encoder.categories_,m2_encoder.categories_))[0]\n",
    "data_y = np.hstack((m1_feature,m2_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_y:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_y = pd.DataFrame(columns = feature_names_y,data=data_y)\n",
    "train_input_fn_y = ExpUtils.make_input_fn(data_y, toy_data['y_lbl'])\n",
    "eval_input_fn_y = ExpUtils.make_input_fn(data_y, toy_data['y_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "y_m1_m2_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=2)\n",
    "y_m1_m2_est.train(train_input_fn_y)\n",
    "result = y_m1_m2_est.evaluate(eval_input_fn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results from TF\n",
    "print(result)\n",
    "print('*****************************')\n",
    "pred_dicts = list(y_m1_m2_est.predict(eval_input_fn_y))\n",
    "preds = [int(pred['classes'][0]) for pred in pred_dicts]\n",
    "\n",
    "print(classification_report(toy_data['y_lbl'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the configuration per experiment\n",
    "conf_df = network_structure.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''add the confirguration for each of the variables in the graph:\n",
    "    - estimator: if applicable, the conditional pdf estimated as a TF model\n",
    "    - prior: the prior distribution of the variable\n",
    "    - encoder: if applicable, the encoder assocaited with the variable\n",
    "    - feature_name: if applicable, the names of the input features for the model associated with the model\n",
    "'''\n",
    "estimators_ = ['','','','m1_x1_x2_est','m2_x2_x3_est','y_m1_m2_est']\n",
    "priors_ = ['x1_prior','x2_prior','x3_prior','m1_prior','m2_prior','y_prior']\n",
    "encoders_ = ['x1_encoder','x2_encoder','x3_encoder','m1_encoder','m2_encoder','y_encoder']\n",
    "feature_names_ = ['','','','feature_names_m1','feature_names_m2','feature_names_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df['estimator'] = estimators_\n",
    "conf_df['prior'] = priors_\n",
    "conf_df['encoder'] = encoders_\n",
    "conf_df['feature_names'] = feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Eq 1 in the paper\n",
    "def joint_prob(df,conf_df, head):\n",
    "    if head not in set(conf_df['node']):\n",
    "        print('head is not a node')\n",
    "        return\n",
    "    # get dependencies\n",
    "    current_node = conf_df[conf_df['node']==head]\n",
    "    prior_str = current_node['prior'].iloc[0]\n",
    "    if ''!=prior_str:\n",
    "        prior_dist = eval(current_node['prior'].iloc[0])\n",
    "    else:\n",
    "        prior_dist = None\n",
    "    dep_nodes = current_node.parent_node.str.split(',').tolist()[0]\n",
    "    if len(dep_nodes)==1 and dep_nodes[0]=='':#no dependencies end of recurssion\n",
    "        # encode the features and then calcualte the priors if they are defined\n",
    "        if prior_dist is not None:\n",
    "            #load the encoder and encode the input data\n",
    "            encoder_ = eval(current_node['encoder'].iloc[0])\n",
    "            encoded_features = encoder_.transform(np.asarray(df[head]).reshape(-1,1)).toarray()\n",
    "            tmp = tf.convert_to_tensor(np.tile(prior_dist.prob(encoded_features),(encoded_features.shape[1],1)).T) \n",
    "            output = tf.convert_to_tensor(encoded_features,dtype=tf.float32)* tmp \n",
    "            return output\n",
    "        return 1.0\n",
    "    n_probs = None\n",
    "    for n in dep_nodes:\n",
    "        probs_ = joint_prob(df,conf_df, head=n)\n",
    "        if n_probs is None:\n",
    "            n_probs = probs_\n",
    "        else:\n",
    "            n_probs = tf.concat([n_probs,probs_],axis=1)\n",
    "    est = eval(current_node['estimator'].iloc[0])\n",
    "    feature_names = eval(current_node['feature_names'].iloc[0])\n",
    "    eval_input_fn = ExpUtils.make_input_fn(pd.DataFrame(columns=feature_names,data=n_probs.numpy()),\n",
    "                                               None,num_epochs=1, shuffle=False)\n",
    "    cond = ExpUtils.conditional_prob(eval_input_fn,est)\n",
    "    \n",
    "    if prior_dist is not None:\n",
    "        prior = prior_dist.prob(cond)\n",
    "        return tf.tensordot(cond,tf.reduce_mean(prior,axis=0),axes=0)\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the code by calculating the joing probability of the data\n",
    "r= joint_prob(toy_data,conf_df, head='y')\n",
    "plt.hist(tf.transpose(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some data and calculate the joint distribution\n",
    "def one_run_simulation(df,conf_df,dists,sample_no=10000,head='y',features=['x1','x2','x3']):\n",
    "    #sample all the features\n",
    "    sim_data = pd.DataFrame(columns=features)\n",
    "    for feature in features:\n",
    "        #sample from the feature distribution\n",
    "        column =  conf_df[conf_df['node']==feature]\n",
    "        feature_samples = dists[feature].sample(sample_no)\n",
    "        categories_ = eval(column['encoder'].iloc[0]).categories_[0]\n",
    "        sim_data[feature] = categories_[np.argmax(feature_samples,axis=1)]\n",
    "    return joint_prob(sim_data,conf_df, head='y')\n",
    "\n",
    "# run the simulations muliple times\n",
    "def repeated_sim(df,conf_df,dists,head='y',features=['x1','x2','x3'],n_samples=10000, repeats=100):\n",
    "    results=[]\n",
    "    \n",
    "    for r in tqdm(range(repeats)):\n",
    "        result= one_run_simulation(df,conf_df,dists,sample_no=n_samples,head='y',features=features)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Baseline output of the model without any modifications to the distributions\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist}\n",
    "\n",
    "results = repeated_sim(toy_data,conf_df,dists) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Change the distribution of x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_dist_exp1 = tfd.RelaxedOneHotCategorical(temperature, probs=[.1,.2,.7])\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist_exp1}\n",
    "results_exp1 = repeated_sim(toy_data,conf_df,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add functions to facilitate comapriosons between baseline and experiments\n",
    "\n",
    "def make_cat_dist(x):\n",
    "    binary_results = np.argmax(x,axis=1)\n",
    "    probs = np.array([np.sum(binary_results==0), np.sum(binary_results==1)])/binary_results.shape[0]\n",
    "    return tfd.Categorical(probs=probs)\n",
    "\n",
    "def kl_divergence(p,q, bins):\n",
    "    def get_norm_bins (x,bins):\n",
    "        bin_values= np.histogram(np.vstack(x),bins =bins)[0]\n",
    "        return bin_values/np.sum(bin_values)\n",
    "    p_bins = get_norm_bins(p,bins)\n",
    "    q_bins = get_norm_bins(q,bins)\n",
    "    result = np.sum(np.where((q_bins!=0) & (p_bins!=0), p_bins*np.log(p_bins/q_bins),0))\n",
    "    return result,p_bins,q_bins\n",
    "\n",
    "def compare_results(results_1,results_2,bins=30, color=None):\n",
    "    plt_results1 = np.vstack(results_1)\n",
    "    plt_results2 = np.vstack(results_2)\n",
    "    sns.distplot(plt_results1[:,0],bins=bins,kde=True,norm_hist=True)\n",
    "    sns.distplot(plt_results2[:,0],bins=bins,kde=True,norm_hist=True,color=color)\n",
    "    plt.ylim([0,100])\n",
    "    dist_kl = tfp.distributions.kl_divergence(make_cat_dist(plt_results1),make_cat_dist(plt_results2))\n",
    "    bin_kl = kl_divergence(results_1,results_2,bins)\n",
    "    return dist_kl,bin_kl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp1))\n",
    "plt.legend(['baseline','exp_1'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Change the distribution of x3 and x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_dist_exp2 = tfd.RelaxedOneHotCategorical(temperature, probs=[.1,.2,.7])\n",
    "x1_dist_exp2 = tfd.RelaxedOneHotCategorical(temperature, probs=[.9,.1])\n",
    "dists={'x1':x1_dist_exp2,'x2':x2_dist,'x3':x3_dist_exp2}\n",
    "results_exp2 = repeated_sim(toy_data,conf_df,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp2,color='r'))\n",
    "plt.legend(['baseline','exp_2'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Change the distribution of x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_dist_exp3 = tfd.RelaxedOneHotCategorical(temperature, probs=[.6,.3,.1])\n",
    "dists={'x1':x1_dist,'x2':x2_dist_exp3,'x3':x3_dist}\n",
    "results_exp3 = repeated_sim(toy_data,conf_df,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp3,color='y'))\n",
    "plt.legend(['baseline','exp_3'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Change the distribution of x1, x2, and x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists={'x1':x1_dist_exp2,'x2':x2_dist_exp3,'x3':x3_dist_exp2}\n",
    "results_exp4 = repeated_sim(toy_data,conf_df,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp4,color='g'))\n",
    "plt.legend(['baseline','exp_4'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Change the classifier for m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_feature = x1_encoder.transform(np.asarray(toy_data['x1']).reshape(-1,1)).toarray()\n",
    "x2_feature = x2_encoder.transform(np.asarray(toy_data['x2']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_m1 = np.hstack((x1_encoder.categories_,x2_encoder.categories_))[0]\n",
    "data_m1 = np.hstack((x1_feature,x2_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m1:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m1 = pd.DataFrame(columns = feature_names_m1,data=data_m1)\n",
    "train_input_fn = ExpUtils.make_input_fn(data_m1, toy_data['m1_lbl'])\n",
    "eval_input_fn = ExpUtils.make_input_fn(data_m1, toy_data['m1_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "#DNN classifier\n",
    "test_DNN_estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[20, 10],n_classes=3)\n",
    "\n",
    "test_DNN_estimator.train(train_input_fn)\n",
    "metrics = test_DNN_estimator.evaluate(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_5 = conf_df.copy()\n",
    "conf_df_5.iloc[3]['estimator'] = 'test_DNN_estimator'\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist}\n",
    "\n",
    "results_exp5 = repeated_sim(toy_data,conf_df_5,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp5,color='c'))\n",
    "plt.legend(['baseline','exp_5'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Change the classifier for m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a linear classifier with random forest\n",
    "# build p(x2|f2,f3) \n",
    "x2_feature = x2_encoder.transform(np.asarray(toy_data['x2']).reshape(-1,1)).toarray()\n",
    "x3_feature = x3_encoder.transform(np.asarray(toy_data['x3']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_m2 = np.hstack((x2_encoder.categories_,x3_encoder.categories_))[0]\n",
    "data_m2 = np.hstack((x2_feature,x3_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m2:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m2 = pd.DataFrame(columns = feature_names_m2,data=data_m2)\n",
    "\n",
    "\n",
    "train_input_fn_m2 = ExpUtils.make_input_fn(data_m2, toy_data['m2_lbl'])\n",
    "eval_input_fn_m2 = ExpUtils.make_input_fn(data_m2, toy_data['m2_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "tree_est = tf.estimator.BoostedTreesClassifier(feature_columns=feature_columns,\n",
    "                                               n_batches_per_layer=int(0.5*data_m2.shape[0]/10))\n",
    "tree_est.train(train_input_fn_m2)\n",
    "metrics = tree_est.evaluate(eval_input_fn_m2)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_6 = conf_df.copy()\n",
    "conf_df_6.iloc[4]['estimator'] = 'tree_est'\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist}\n",
    "\n",
    "results_exp6 = repeated_sim(toy_data,conf_df_6,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp6,color='m'))\n",
    "plt.legend(['baseline','exp_6'])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Replace m1 and m2 with random classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7\n",
    "# replace a classifier with a random output\n",
    "\n",
    "# build p(m2|x2,x3) \n",
    "data_m2 = np.hstack((x2_feature,x3_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m2:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m2 = pd.DataFrame(columns = feature_names_m2,data=data_m2)\n",
    "\n",
    "random_lbl = toy_data['m2_lbl'][np.random.permutation(toy_data.shape[0])]\n",
    "train_input_fn_m2 = ExpUtils.make_input_fn(data_m2, random_lbl)\n",
    "eval_input_fn_m2 = ExpUtils.make_input_fn(data_m2, random_lbl, num_epochs=1, shuffle=False)\n",
    "\n",
    "random_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=2)\n",
    "random_est.train(train_input_fn_m2)\n",
    "result = random_est.evaluate(eval_input_fn_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_7a = conf_df.copy()\n",
    "conf_df_7a.iloc[4]['estimator'] = 'random_est'\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist}\n",
    "\n",
    "results_exp7a = repeated_sim(toy_data,conf_df_7a,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp7a,color='#FFA500'))\n",
    "plt.legend(['baseline','Randomly trained $m_2$'])\n",
    "plt.xlim([0.39,0.55])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_7a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7b\n",
    "# replace the other classifier with a random output\n",
    "\n",
    "# build p(m1|x1,x2) \n",
    "feature_names_m1 = np.hstack((x1_encoder.categories_,x2_encoder.categories_))[0]\n",
    "data_m1 = np.hstack((x1_feature,x2_feature))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_m1:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_m1 = pd.DataFrame(columns = feature_names_m1,data=data_m1)\n",
    "\n",
    "random_lbl = toy_data['m2_lbl'][np.random.permutation(toy_data.shape[0])]\n",
    "train_input_fn = ExpUtils.make_input_fn(data_m1, random_lbl)\n",
    "eval_input_fn = ExpUtils.make_input_fn(data_m1, random_lbl, num_epochs=1, shuffle=False)\n",
    "\n",
    "random_est_m1 = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=3)\n",
    "random_est_m1.train(train_input_fn)\n",
    "result = random_est_m1.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_7b = conf_df.copy()\n",
    "conf_df_7b.iloc[3]['estimator'] = 'random_est_m1'\n",
    "dists={'x1':x1_dist,'x2':x2_dist,'x3':x3_dist}\n",
    "\n",
    "results_exp7b = repeated_sim(toy_data,conf_df_7b,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp7b,color='#FF6347'))\n",
    "plt.legend(['baseline','Randomly trained $m_1$'])\n",
    "plt.xlim([0.39,0.55])\n",
    "plt.xlabel('Solution Output Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_7b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
