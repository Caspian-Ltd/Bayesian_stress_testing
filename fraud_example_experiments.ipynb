{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from utils import GraphUtils, ExpUtils\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from tensorflow.python import tf2\n",
    "if not tf2.enabled():\n",
    "    import tensorflow.compat.v2 as tf\n",
    "    tf.enable_v2_behavior()\n",
    "    assert tf2.enabled()\n",
    "from tqdm import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context='talk',font_scale=0.7)\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original data are available here (https://www.kaggle.com/ntnu-testimon/banksim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from a local copy\n",
    "df_bank = pd.read_csv('data\\\\bs140513_032310.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "remove the zip code information as it is the same across all the customers\n",
    "\n",
    "clean the strings from category, merchant, age, gender, and customer fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_bank['zipMerchant']\n",
    "del df_bank['zipcodeOri']\n",
    "\n",
    "df_bank['category']=df_bank['category'].str.strip(\"'\")\n",
    "df_bank['merchant']=df_bank['merchant'].str.strip(\"'\")\n",
    "df_bank['age']=df_bank['age'].str.strip(\"'\")\n",
    "df_bank['gender']=df_bank['gender'].str.strip(\"'\")\n",
    "df_bank['customer']=df_bank['customer'].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract additional features\n",
    "- risk of nature of business: measure for each business type the probability of its transaction to be fraud\n",
    "- amount context: output the p-value of the amount after all the customer transactions are fitted to a lognorm distribution\n",
    "- frequency: fit all the frequencies across the dataset with a lognorm distribution and then measure the probability of frequency of a given transaction (frequent or not frequent states only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature of Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk of nature of business\n",
    "nature_of_bus_lbl = LabelEncoder().fit(df_bank['category'])\n",
    "\n",
    "#calculate nature of business risk\n",
    "risk_per_nob = {}\n",
    "for b in df_bank.category.unique():\n",
    "    t_df = df_bank[df_bank['category']==b]\n",
    "    ratio = t_df[t_df['fraud']==1].shape[0] / t_df.shape[0]\n",
    "    risk_per_nob[b] = ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nob_risks=np.asarray(list(risk_per_nob.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_to_label(risk):\n",
    "    if risk <=0.2:\n",
    "        if np.random.rand() >0.1:\n",
    "            return 'low'\n",
    "    if risk<=0.7:\n",
    "        if np.random.rand() >0.1:\n",
    "            return 'medium'\n",
    "    return 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nob = df_bank[['amount','category']]\n",
    "df_nob['risk_label']=df_nob['category'].apply(lambda x :risk_to_label(risk_per_nob[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier for nature of business risk\n",
    "nob_cat_encoder = OneHotEncoder().fit(np.asarray(df_nob['category']).reshape(-1,1))\n",
    "\n",
    "df_nob['lbl'] =  LabelEncoder().fit_transform(df_nob['risk_label'])\n",
    "cat_feature = nob_cat_encoder.transform(np.asarray(df_nob['category']).reshape(-1,1)).toarray()\n",
    "\n",
    "feature_names_nob = np.append(nob_cat_encoder.categories_,['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_nob = np.hstack((cat_feature,np.asarray(df_nob['amount']).reshape(-1,1)))\n",
    "\n",
    "feature_columns=[]\n",
    "for feature_name in feature_names_nob:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))\n",
    "data_nob = pd.DataFrame(columns = feature_names_nob,data=data_nob)\n",
    "train_input_fn_nob = ExpUtils.make_input_fn(data_nob, df_nob['lbl'])\n",
    "eval_input_fn_nob = ExpUtils.make_input_fn(data_nob, df_nob['lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "nob_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=3)\n",
    "nob_est.train(train_input_fn_nob)\n",
    "nob_est.evaluate(eval_input_fn_nob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dicts = list(nob_est.predict(eval_input_fn_nob))\n",
    "preds = [int(pred['classes'][0]) for pred in pred_dicts]\n",
    "\n",
    "print(classification_report(df_nob['lbl'],preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the prior distribution\n",
    "amounts = df_bank['amount']\n",
    "from scipy.stats import norm,kstest,lognorm\n",
    "param = norm.fit(amounts[amounts<5000])\n",
    "amounts_prior = tfd.TruncatedNormal(loc=tf.reduce_mean(amounts[amounts<5000]),\n",
    "                                    scale =tf.math.reduce_std(amounts[amounts<5000]),low=0., high=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = amounts_prior.sample(10000)\n",
    "plt.hist(o,density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_features(transaction,cust_df):\n",
    "    #count the number of times the cp has been transacted with before\n",
    "    mer = transaction['merchant']\n",
    "    mer_data = cust_df[cust_df['merchant']==mer]\n",
    "    \n",
    "    tr_count = mer_data.shape[0]\n",
    "    mer_time_stamps = np.array(mer_data['step'])\n",
    "    avg_amount = np.mean(mer_data['amount'])\n",
    "    std_amount = np.std(mer_data['amount'])\n",
    "    \n",
    "    return tr_count,mer_time_stamps,avg_amount,std_amount,mer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is only necessary if you want to re-collect and label the frequency data \n",
    "# otherwise laod the labelled frequency data\n",
    "\n",
    "relabel_frequency=False\n",
    "\n",
    "if not relabel_frequency:\n",
    "    sample_freq_data = pd.read_csv('data/ferquency_training_data.csv')\n",
    "else:\n",
    "    #collect all the frequency data\n",
    "    n = 200\n",
    "    sample_freq_data = df_bank.sample(n,)\n",
    "    freq_labels = np.zeros(n)\n",
    "    i=0\n",
    "    for idx, sample in sample_freq_data.iterrows():\n",
    "        cust_df = df_bank[df_bank['customer']==sample['customer']]\n",
    "        tr_count,mer_time_stamps,avg_amount,std_amount,mer_data = extract_frequency_features(sample,cust_df)\n",
    "        avg_time = 0\n",
    "        std_time = 0\n",
    "        if len(mer_time_stamps)>1:\n",
    "            avg_time = np.mean(np.diff(mer_time_stamps))\n",
    "            std_time = np.std(np.diff(mer_time_stamps))\n",
    "        print('index:',i,'\\n',\n",
    "          'all transactions', cust_df.shape[0],'\\n'\n",
    "          'Count:',tr_count,'\\n',\n",
    "          #'time_stamps:',mer_time_stamps,'\\n',\n",
    "          'average_time:',avg_time,'\\n',\n",
    "          'std_time:',std_time,'\\n',\n",
    "          'average amount:',avg_amount,'\\n',\n",
    "          'std amount:',std_amount)\n",
    "        x = input('Enter 0-infrequent, 1- rare, 2- regular:\\n')\n",
    "        if x=='q':\n",
    "            break\n",
    "        freq_labels[i]=int(x)\n",
    "        i = i+1\n",
    "    sample_freq_data['freq_lbl']=freq_labels.astype(int)\n",
    "    sample_freq_data['frequency'] = np.asarray(['infrequent','rare','regular'])[freq_labels.astype(int)]\n",
    "    sample_freq_data.to_csv('ferquency_training_data_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data=np.zeros((sample_freq_data.shape[0],5))\n",
    "i=0\n",
    "for idx, sample in sample_freq_data.iterrows():\n",
    "    cust_df = df_bank[df_bank['customer']==sample['customer']]\n",
    "    tr_count,mer_time_stamps,avg_amount,std_amount,mer_data = extract_frequency_features(sample,cust_df)\n",
    "    avg_time = 0\n",
    "    std_time = 0\n",
    "    if len(mer_time_stamps)>1:\n",
    "        avg_time = np.mean(np.diff(mer_time_stamps))\n",
    "        std_time = np.std(np.diff(mer_time_stamps))\n",
    "    freq_data[i,0]=avg_time\n",
    "    freq_data[i,1]=std_time\n",
    "    freq_data[i,2]=avg_amount\n",
    "    freq_data[i,3]=std_amount\n",
    "    freq_data[i,4]=tr_count/cust_df.shape[0]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate n points of the distrubtion to be able to measure the probability of a point\n",
    "def estimate_probability(_param,sample,n=10000):\n",
    "    samples = scipy.stats.lognorm.rvs(*_param[:-2], \n",
    "                       loc=_param[-2], scale=_param[-1], size=n)\n",
    "    return len(samples[samples<sample])/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender encoder\n",
    "gender_encoder = LabelEncoder().fit(df_bank['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features :\n",
    "- age \n",
    "- gender\n",
    "- category\n",
    "- amount\n",
    "- avgerage time between transactions with a given merchant(avg_tbt)\n",
    "- stddev time between transactions with a given merchant(std_tbt)\n",
    "- avgerage transaction amount with a given merchant(avg_amt)\n",
    "- stddev transaction amount with a given merchant(std_amt)\n",
    "- ratio of number of transactions with a given merchant to the overall transactions number (t_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed the feature extraction we will cache some of the calculations\n",
    "\n",
    "cache_amount={}\n",
    "cache_customer={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(customer_df,merchant,category,amount):\n",
    "    \n",
    "    customer_name = customer_df['customer'].unique()[0]\n",
    "    #add the age,gender features\n",
    "    age = list(customer_df['age'])[0]\n",
    "    if age =='U':\n",
    "        age = 7\n",
    "    else:\n",
    "        age = int(age)\n",
    "    features = [age,\n",
    "                gender_encoder.transform([list(customer_df['gender'])[0]])[0]]\n",
    "    \n",
    "    #add the nature of business and its risk\n",
    "    features.extend(nob_cat_encoder.transform(np.asarray([category]).reshape(-1,1)).toarray()[0])\n",
    "    #add the amount context\n",
    "    if customer_df.shape[0] < 10:\n",
    "        ratio = amount/np.sum(customer_df['amount'])\n",
    "        features.append(ratio)\n",
    "    else:\n",
    "        param = cache_amount.get(customer_name)\n",
    "        if param is None:\n",
    "            param = scipy.stats.lognorm.fit(customer_df['amount'])\n",
    "            cache_amount[customer_name] = param\n",
    "        features.append(estimate_probability(param,amount,n=1000))\n",
    "        \n",
    "    #add the frequency features\n",
    "    #count the number of times the cp has been transacted with before\n",
    "    key = customer_name+'_'+merchant\n",
    "    cached = cache_customer.get(key)\n",
    "    if cached is not None:\n",
    "        [avg_tbt,std_tbt,avg_amt,std_amt,t_count] = cached\n",
    "    else:\n",
    "        mer_data = customer_df[customer_df['merchant']==merchant]\n",
    "    \n",
    "        mer_time_stamps = np.array(mer_data['step'])\n",
    "        avg_tbt = 0\n",
    "        std_tbt = 0\n",
    "        if len(mer_time_stamps)>1:\n",
    "            avg_tbt = np.mean(np.diff(mer_time_stamps))\n",
    "            std_tbt = np.std(np.diff(mer_time_stamps))\n",
    "        \n",
    "        avg_amt = np.mean(mer_data['amount'])\n",
    "        std_amt = np.std(mer_data['amount'])\n",
    "    \n",
    "        t_count = mer_data.shape[0]/customer_df.shape[0]\n",
    "        cache_customer[key] = [avg_tbt,std_tbt,avg_amt,std_amt,t_count]\n",
    "        \n",
    "    features.extend([avg_tbt,std_tbt,avg_amt,std_amt,t_count])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test feature extraction\n",
    "customer_df = df_bank[df_bank['customer']=='C352968107']\n",
    "\n",
    "feature_extraction(customer_df,'M348934600','es_transportation',39.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_all = ['age','gender']\n",
    "feature_names_all.extend(nob_cat_encoder.categories_[0])\n",
    "feature_names_all.extend(['amount','avg_tbt','std_tbt','avg_amt','std_amt','t_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the graph and then stress test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = GraphUtils.load_graph(r\"graph_structures/fraud_example_structure.xlsx\")\n",
    "GraphUtils.visualise_network(network_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "load_data = True #load all training data only set to false if you require re-running the feature extraction code.\n",
    "if load_data:\n",
    "    data = pd.read_csv('data/training_data.csv')\n",
    "    del data['Unnamed: 0']\n",
    "else:\n",
    "    #run the feature extraction on all the transactions\n",
    "    feature_data=[]\n",
    "    \n",
    "    with tqdm(total=df_bank.shape[0]) as pbar:\n",
    "        for row in df_bank.iterrows():\n",
    "            customer = row[1]['customer']\n",
    "            merchant = row[1]['merchant']\n",
    "            category = row[1]['category']\n",
    "            amount   = row[1]['amount']\n",
    "            customer_df = df_bank[df_bank['customer']==customer]\n",
    "\n",
    "            row_features = feature_extraction(customer_df,merchant,category,amount)\n",
    "            feature_data.append(row_features)\n",
    "            #time.sleep(0.01)\n",
    "            pbar.update(1)\n",
    "    data = pd.DataFrame(columns=feature_names_all,data=feature_data)\n",
    "    data['label'] = df_bank['fraud']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the nature of business classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sampled = data[data['label']==1]\n",
    "sub_sampled = pd.concat((sub_sampled,data[data['label']==0].sample(7200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sampled_labels=np.ones(7200,dtype=int)\n",
    "sub_sampled_labels=np.append(sub_sampled_labels, np.zeros(7200,dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nob_onehot_encoder = OneHotEncoder().fit(np.array(df_bank['category']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nature of business classifier\n",
    "feature_columns_nob=[]\n",
    "for feature_name in feature_names_nob:\n",
    "    feature_columns_nob.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float64))\n",
    "data_nob = data[feature_names_nob]\n",
    "train_input_fn_nob = ExpUtils.make_input_fn(data_nob, df_nob['lbl'])\n",
    "eval_input_fn_nob = ExpUtils.make_input_fn(data_nob, df_nob['lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "nob_est = tf.estimator.LinearClassifier(feature_columns=feature_columns_nob,n_classes=3)\n",
    "nob_est.train(train_input_fn_nob)\n",
    "nob_est.evaluate(eval_input_fn_nob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the frequency classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the frequency classifier\n",
    "\n",
    "feature_names_freq = ['avg_tbt','std_tbt','avg_amt','std_amt','t_count']\n",
    "feature_columns_freq=[]\n",
    "for feature_name in feature_names_freq:\n",
    "    feature_columns_freq.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float64))\n",
    "data_frequency = pd.DataFrame(columns = feature_names_freq,data=freq_data)\n",
    "\n",
    "train_input_fn_freq = ExpUtils.make_input_fn(data_frequency, sample_freq_data['freq_lbl'])\n",
    "eval_input_fn_freq = ExpUtils.make_input_fn(data_frequency, sample_freq_data['freq_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "freq_est = tf.estimator.LinearClassifier(feature_columns=feature_columns_freq,n_classes=3)\n",
    "freq_est.train(train_input_fn_freq)\n",
    "freq_est.evaluate(eval_input_fn_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the decision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predcitions from the freq and nob estimators to build the decision classifier\n",
    "input_fn_ = ExpUtils.make_input_fn(data, None, num_epochs=1, shuffle=False)\n",
    "freq_preds = ExpUtils.get_class_probs(input_fn_,freq_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nob_preds = ExpUtils.get_class_probs(input_fn_,nob_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_decision=['age','gender','amount']\n",
    "feature_names_decision.extend(['infrequent','rare','regular','high', 'low', 'medium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['infrequent'] = freq_preds[:,0]\n",
    "data['rare'] = freq_preds[:,1]\n",
    "data['regular'] = freq_preds[:,2]\n",
    "\n",
    "data['high'] = nob_preds[:,0]\n",
    "data['low'] = nob_preds[:,1]\n",
    "data['medium'] = nob_preds[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'] = df_bank['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_data:\n",
    "    data.to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_columns_decision=[]\n",
    "for feature_name in feature_names_decision:\n",
    "    feature_columns_decision.append(tf.feature_column.numeric_column(feature_name,\n",
    "                                           dtype=tf.float64))\n",
    "data_decision = sub_sampled[feature_names_decision]\n",
    "\n",
    "train_input_fn_dec = ExpUtils.make_input_fn(data_decision, sub_sampled_labels)\n",
    "eval_input_fn_dec = ExpUtils.make_input_fn(data_decision, sub_sampled_labels, num_epochs=1, shuffle=False)\n",
    "\n",
    "dec_est = tf.estimator.LinearClassifier(feature_columns=feature_columns_decision,n_classes=2)\n",
    "dec_est.train(train_input_fn_dec)\n",
    "dec_est.evaluate(eval_input_fn_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Priors and Feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the priors and \n",
    "nob_prior = tfd.OneHotCategorical(probs=[.1,.88,.2]) \n",
    "freq_prior = tfd.OneHotCategorical(probs=[.4,.15,.45]) \n",
    "dec_prior = tfd.OneHotCategorical(probs=[.98,.02])\n",
    "amount_prior = tfd.TruncatedNormal(loc=0.6,scale =0.27,low=0., high=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_freq = df_bank['category'].value_counts(normalize=True)[['es_barsandrestaurants',\n",
    " 'es_contents',\n",
    " 'es_fashion',\n",
    " 'es_food',\n",
    " 'es_health',\n",
    " 'es_home',\n",
    " 'es_hotelservices',\n",
    " 'es_hyper',\n",
    " 'es_leisure',\n",
    " 'es_otherservices',\n",
    " 'es_sportsandtoys',\n",
    " 'es_tech',\n",
    " 'es_transportation',\n",
    " 'es_travel',\n",
    " 'es_wellnessandbeauty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the distributions\n",
    "temperature = .7\n",
    "age_dist = tfd.Categorical(probs=data['age'].value_counts(normalize=True, sort=False))\n",
    "gender_dist = tfd.Categorical(probs=data['gender'].value_counts(normalize=True, sort=False))\n",
    "category_dist = tfd.RelaxedOneHotCategorical(temperature, probs=cat_freq)\n",
    "amount_dist = tfd.TruncatedNormal(loc=0.6,scale =0.27,low=0., high=1.)\n",
    "std_tbt_dist = tfd.Gamma(concentration=0.395,rate = 0.1) # params estimated using scipy\n",
    "avg_tbt_dist = tfd.Gamma(concentration=0.1277,rate = 0.04)\n",
    "\n",
    "avg_amt_dist = tfd.Gamma(concentration=2.7,rate = 0.1)\n",
    "std_amt_dist = tfd.Gamma(concentration=3.5,rate = 0.2)\n",
    "t_count_dist = tfd.TruncatedNormal(loc=0.6,scale =0.27,low=0., high=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preapre the configuration to start the experimentation.\n",
    "conf_df = network_structure.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age']=data['age'].astype(float)\n",
    "data['gender']=data['gender'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the confirguration\n",
    "estimators_ = ['','','','','','','','','','freq_est','nob_est','dec_est']\n",
    "priors_ = ['','','','amount_prior','','','','','','freq_prior','nob_prior','dec_prior']\n",
    "encoders_ = ['','','nob_onehot_encoder','','','','','','','','','']\n",
    "feature_names_ = ['','','','','','','','','','feature_names_freq','feature_names_nob','feature_names_decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df['estimator'] = estimators_\n",
    "conf_df['prior'] = priors_\n",
    "conf_df['encoder'] = encoders_\n",
    "conf_df['feature_names'] = feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Eq 1\n",
    "def joint_prob(df,conf_df, head):\n",
    "    \n",
    "    if head not in set(conf_df['node']):\n",
    "        print('head is not a node')\n",
    "        return\n",
    "    # get dependencies\n",
    "    current_node = conf_df[conf_df['node']==head]\n",
    "    prior_str = current_node['prior'].iloc[0]\n",
    "    if ''!=prior_str:\n",
    "        prior_dist = eval(prior_str)\n",
    "    else:\n",
    "        prior_dist = None\n",
    "        \n",
    "    dep_nodes = current_node.parent_node.str.split(',').tolist()[0]\n",
    "    \n",
    "    if len(dep_nodes)==1 and dep_nodes[0]=='':#no dependencies end of recurssion\n",
    "        \n",
    "        encoded_features = None\n",
    "        #load the encoder and encode the input data if required\n",
    "        if ''!= current_node['encoder'].iloc[0]:  \n",
    "            encoder_ = eval(current_node['encoder'].iloc[0])\n",
    "            encoded_features = encoder_.transform(np.asarray(df[head]).reshape(-1,1)).toarray() \n",
    "            # encode the features and then calcualte the priors if they are defined\n",
    "            if prior_dist is not None:    \n",
    "                tmp = tf.convert_to_tensor(np.tile(prior_dist.prob(encoded_features),(encoded_features.shape[1],1)).T) \n",
    "                return tf.convert_to_tensor(encoded_features,dtype=tf.float64)* tmp\n",
    "            else: \n",
    "                return encoded_features\n",
    "        else:\n",
    "            if prior_dist is not None:\n",
    "                return prior_dist.prob(df[head])\n",
    "            else:\n",
    "                return tf.cast(df[head],tf.float32)\n",
    "\n",
    "    n_probs = None\n",
    "    for n in dep_nodes:\n",
    "        probs_ = joint_prob(df,conf_df, head=n)\n",
    "        if len(probs_.shape) ==1:\n",
    "            probs_=tf.expand_dims(probs_,1)    \n",
    "        if n_probs is None:\n",
    "            n_probs = probs_\n",
    "        else:\n",
    "            n_probs = tf.concat([n_probs,probs_],axis=1)\n",
    "            \n",
    "                \n",
    "\n",
    "    est = eval(current_node['estimator'].iloc[0])\n",
    "    feature_names = eval(current_node['feature_names'].iloc[0])\n",
    "    eval_input_fn = ExpUtils.make_input_fn(pd.DataFrame(columns=feature_names,data=n_probs.numpy()),\n",
    "                                               None,num_epochs=1, shuffle=False)\n",
    "    cond = ExpUtils.conditional_prob(eval_input_fn,est)\n",
    "    \n",
    "    if prior_dist is not None:\n",
    "        prior = prior_dist.prob(cond)\n",
    "        return tf.tensordot(cond,tf.reduce_mean(prior,axis=0),axes=0)\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the implementation\n",
    "r1= joint_prob(data,conf_df, head='decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(r1[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some data and calculate the joint distribution\n",
    "\n",
    "def one_run_simulation(conf_df,dists,sample_no=10000,head='decision',features=['age',\n",
    " 'gender','category','amount','avg_tbt','std_tbt','avg_amt','std_amt','t_count']):\n",
    "    #sample all the features\n",
    "    sim_data = pd.DataFrame(columns=features)\n",
    "    for feature in features:\n",
    "        #sample from the feature distribution\n",
    "        column =  conf_df[conf_df['node']==feature]\n",
    "        feature_samples = dists[feature].sample(sample_no)\n",
    "        if ''!= column['encoder'].iloc[0]:\n",
    "            categories_ = eval(column['encoder'].iloc[0]).categories_[0]\n",
    "            sim_data[feature] = categories_[np.argmax(feature_samples,axis=1)]\n",
    "        else:\n",
    "            sim_data[feature] = feature_samples\n",
    "    return joint_prob(sim_data,conf_df, head=head)\n",
    "\n",
    "def repeated_sim(conf_df,dists,head='decision',features=['age',\n",
    " 'gender','category','amount','avg_tbt','std_tbt','avg_amt','std_amt','t_count'],n_samples=1000, repeats=100):\n",
    "    results=[]\n",
    "    \n",
    "    for r in tqdm(range(repeats)):\n",
    "        result= one_run_simulation(conf_df,dists,sample_no=n_samples,head=head,features=features)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish baseline and implement comparions functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dists={'age':age_dist,\n",
    "       'gender':gender_dist,\n",
    "       'category':category_dist,\n",
    "       'amount':amount_dist,\n",
    "       'avg_tbt':avg_tbt_dist,\n",
    "       'std_tbt':std_tbt_dist,\n",
    "       'avg_amt':avg_amt_dist,\n",
    "       'std_amt':std_amt_dist,\n",
    "       't_count':t_count_dist}\n",
    "results = repeated_sim(conf_df,dists,n_samples=1000,repeats=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_dist(x):\n",
    "    binary_results = np.argmax(x,axis=1)\n",
    "    probs = np.array([np.sum(binary_results==0), np.sum(binary_results==1)])/binary_results.shape[0]\n",
    "    return tfd.Categorical(probs=probs)\n",
    "def kl_divergence(p,q, bins):\n",
    "    def get_norm_bins (x,bins):\n",
    "        bin_values= np.histogram(np.vstack(x),bins =bins)[0]\n",
    "        return bin_values/np.sum(bin_values)\n",
    "    p_bins = get_norm_bins(p,bins)\n",
    "    q_bins = get_norm_bins(q,bins)\n",
    "    result = np.sum(np.where((q_bins!=0) & (p_bins!=0), p_bins*np.log(p_bins/q_bins),0))\n",
    "    return result,p_bins,q_bins\n",
    "def compare_results(results_1,results_2,bins=30, color=None, label='Experiment'):\n",
    "    plt_results1 = np.vstack(results_1)\n",
    "    plt_results2 = np.vstack(results_2)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['Fraud Probability','Experiment'])\n",
    "    df['Fraud Probability']=np.append(plt_results1[:,1], plt_results2[:,1])\n",
    "    df['Experiment'].iloc[:plt_results1.shape[0]]='Baseline'\n",
    "    df['Experiment'].iloc[plt_results1.shape[0]:]=label\n",
    "    sns.boxplot(x='Experiment',y='Fraud Probability',data=df)\n",
    "    plt.show()\n",
    "    sns.distplot(plt_results1[:,1],bins=bins,kde=True,norm_hist=True)\n",
    "    sns.distplot(plt_results2[:,1],bins=bins,kde=True,norm_hist=True,color=color)\n",
    "    plt.ylim([0,10])\n",
    "    dist_kl = tfp.distributions.kl_divergence(make_cat_dist(plt_results1),make_cat_dist(plt_results2))\n",
    "    bin_kl = kl_divergence(results_1,results_2,bins)\n",
    "    \n",
    "    return dist_kl,bin_kl[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expirment 8: Change the distribution of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_dist_exp8 = tfd.Categorical(probs=[.1,.2,.3,.04,0.06,0.2,.05,0.05])\n",
    "dists_8={'age':age_dist_exp8,\n",
    "       'gender':gender_dist,\n",
    "       'category':category_dist,\n",
    "       'amount':amount_dist,\n",
    "       'avg_tbt':avg_tbt_dist,\n",
    "       'std_tbt':std_tbt_dist,\n",
    "       'avg_amt':avg_amt_dist,\n",
    "       'std_amt':std_amt_dist,\n",
    "       't_count':t_count_dist}\n",
    "results_exp_8 = repeated_sim(conf_df,dists_8,n_samples=1000,repeats=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(age_dist.sample(10000),kde=False,norm_hist=True)\n",
    "sns.distplot(age_dist_exp8.sample(10000),kde=False,norm_hist=True)\n",
    "plt.xlabel('Age Ranges')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(['baseline','Exp_8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp_8,bins=20,label='Exp_8'))\n",
    "plt.legend(['baseline','Exp_8'])\n",
    "plt.xlabel('Fraud Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_fraud_8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment9: Change the distribution of the amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 2 change the distribution of amount\n",
    "amount_dist_exp_9 = tfd.TruncatedNormal(loc=0.1,scale =0.1,low=0., high=1.)\n",
    "dists_9={'age':age_dist,\n",
    "       'gender':gender_dist,\n",
    "       'category':category_dist,\n",
    "       'amount':amount_dist_exp_9,\n",
    "       'avg_tbt':avg_tbt_dist,\n",
    "       'std_tbt':std_tbt_dist,\n",
    "       'avg_amt':avg_amt_dist,\n",
    "       'std_amt':std_amt_dist,\n",
    "       't_count':t_count_dist}\n",
    "results_exp_9 = repeated_sim(conf_df,dists_9,n_samples=1000,repeats=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(amount_dist.sample(1000),norm_hist=True)\n",
    "sns.distplot(amount_dist_exp_9.sample(1000),norm_hist=True,color='r')\n",
    "plt.xlabel('Normalized Transaction Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(['baseline', 'Exp_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp_9,bins=20,label='Exp_9',color='r'))\n",
    "plt.legend(['baseline','Exp_9'])\n",
    "plt.xlabel('Fraud Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_fraud_9.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expierment 10: Replace Frequency and NoB with randomly trained classifeirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 10a train frequency randomly\n",
    "random_lbl = sample_freq_data['freq_lbl'][np.random.permutation(sample_freq_data.shape[0])]\n",
    "train_input_fn_freq = ExpUtils.make_input_fn(data_frequency,random_lbl )\n",
    "eval_input_fn_freq = ExpUtils.make_input_fn(data_frequency, sample_freq_data['freq_lbl'], num_epochs=1, shuffle=False)\n",
    "\n",
    "freq_est_rand = tf.estimator.LinearClassifier(feature_columns=feature_columns_freq,n_classes=3)\n",
    "freq_est_rand.train(train_input_fn_freq)\n",
    "freq_est_rand.evaluate(eval_input_fn_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_10a = conf_df.copy()\n",
    "conf_df_10a.iloc[9]['estimator'] = 'freq_est_rand'\n",
    "\n",
    "results_exp_10a = repeated_sim(conf_df_10a,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_exp_10a,bins=30))\n",
    "plt.legend(['baseline','Exp_10a'])\n",
    "plt.xlabel('Fraud Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('exp_fraud_10a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expeiment 10b train the nature of business classifier on randomly labelled data\n",
    "random_lbl = np.argmax(tfd.OneHotCategorical(probs=[.3,.3,.4]).sample(df_nob['lbl'].shape[0]),axis=1)\n",
    "train_input_fn_nob_rand = ExpUtils.make_input_fn(data_nob, random_lbl)\n",
    "eval_input_fn_nob_rand = ExpUtils.make_input_fn(data_nob, random_lbl, num_epochs=1, shuffle=False)\n",
    "\n",
    "nob_est_rand = tf.estimator.LinearClassifier(feature_columns=feature_columns_nob,n_classes=3)\n",
    "nob_est_rand.train(train_input_fn_nob_rand)\n",
    "nob_est_rand.evaluate(eval_input_fn_nob_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_df_10b = conf_df.copy()\n",
    "conf_df_10b.iloc[10]['estimator'] = 'nob_est_rand'\n",
    "\n",
    "results_10b = repeated_sim(conf_df_10b,dists,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_results(results,results_10b,bins=30))\n",
    "plt.legend(['baseline','Exp_10b'])\n",
    "plt.xlabel('Fraud Probability')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_results = np.vstack(results)\n",
    "plt_results10a = np.vstack(results_exp_10a)\n",
    "plt_results10b = np.vstack(results_10b)\n",
    "    \n",
    "df_plt = pd.DataFrame(columns=['Fraud Probability','Experiment'])\n",
    "df_plt['Fraud Probability']=np.append(np.append(plt_results[:,1], plt_results10b[:,1]),plt_results10a[:,1])\n",
    "df_plt['Experiment'].iloc[:plt_results.shape[0]]='Baseline'\n",
    "df_plt['Experiment'].iloc[plt_results.shape[0]:plt_results.shape[0]+plt_results10b.shape[0]]='Random $m_1$'\n",
    "df_plt['Experiment'].iloc[plt_results.shape[0]+plt_results10b.shape[0]:]='Random $m_2$'\n",
    "box_plot = sns.boxplot(x='Experiment',y='Fraud Probability',data=df_plt,palette=\"Set1\")\n",
    "plt.xticks(rotation=20)\n",
    "medians = np.array([np.median(plt_results[:,1]),np.median(plt_results10b[:,1]),np.median(plt_results10a[:,1])])\n",
    "vertical_offset = medians*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output = joint_prob(data,conf_df, head='decision')\n",
    "exp_10a_output = joint_prob(data,conf_df_10a, head='decision')\n",
    "exp_10b_output = joint_prob(data,conf_df_10b, head='decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_plot = pd.DataFrame(columns=['x','y'])\n",
    "joint_plot['baseline'] = base_output[:2000,1]\n",
    "joint_plot['Random_m2'] = exp_10a_output[:2000,1]\n",
    "joint_plot['Random_m1'] = exp_10b_output[:2000,1]\n",
    "sns.jointplot(x=\"baseline\", y=\"Random_m1\", data=joint_plot, kind=\"kde\",);\n",
    "plt.plot([-0.2,-0.2],[1.2,1.2])\n",
    "plt.show()\n",
    "sns.jointplot(x=\"baseline\", y=\"Random_m2\", data=joint_plot, kind=\"kde\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
